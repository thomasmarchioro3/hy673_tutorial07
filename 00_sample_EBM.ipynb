{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)  # PRNG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish activation function\n",
    "\n",
    "$$\n",
    "y = x \\cdot \\text{sigmoid}(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "One of the many possible smooth versions of ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = torch.arange(-5, 5, 0.1)\n",
    "y_range = x_range * torch.sigmoid(x_range)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_range, y_range, '-', color='C1')\n",
    "plt.xlim([x_range[0], x_range[-1]])\n",
    "plt.grid(linestyle=':')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Swish(x)')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual gradients with Pytorch\n",
    "\n",
    "Suppose we have some function \n",
    "$$\n",
    "y = f(x), x \\in \\mathbb{R}^{d}\n",
    "$$\n",
    "\n",
    "and we want to compute\n",
    "$$\n",
    "\\nabla_x y = \\left[ \\frac{\\partial y}{\\partial x_1}, \\cdots,  \\frac{\\partial y}{\\partial x_1} \\right]^{\\top}\n",
    "$$\n",
    "\n",
    "This can be done by using <code>torch.autograd.grad</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(3, dtype=float)\n",
    "x.requires_grad = True\n",
    "\n",
    "y = torch.sum(x**2)\n",
    "\n",
    "grad, = torch.autograd.grad(y, x)  # gradient of y w.r.t. x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess the result?\n",
    "print(grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling via Langevin Monte Carlo Markov Chain\n",
    "\n",
    "Iterative sampling starting from uniform noise $x(0)\\sim \\mathcal{U}(-1, 1)$:\n",
    "\n",
    "$$\n",
    "x(t+1) = x(t) + \\epsilon\\cdot \\nabla_{x(t)} f_{\\theta}(x(t)) + z(t)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "z(t) \\sim(0, 2\\epsilon I)\n",
    "$$\n",
    "\n",
    "In practice we tune the standard deviation of $z(t)$ manually.\n",
    "\n",
    "Default version of the model is very unstable\n",
    "Tweaks to stabilize the training:\n",
    "- clip the gradient $\\nabla_{x(t)} f_{\\theta}(x(t))$ (either values or norm)\n",
    "- clip the values of $x(t)$ after applying the noise and after applying the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(model, x_hat, steps=60, step_size=10):\n",
    "    \"\"\"\n",
    "    Simplified version of the generate_samples function in utils.py\n",
    "    \"\"\"\n",
    "    # Before MCMC, set model parameters to \"required_grad=False\"\n",
    "    # since we are only interested in the gradients w.r.t. x\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    x_hat.requires_grad = True\n",
    "\n",
    "    # allocate a tensor to generate noise each loop iteration\n",
    "    # more efficient than creating a new tensor every iteration\n",
    "    noise = torch.randn(x_hat.shape, device=x_hat.device)\n",
    "\n",
    "    x_steps = []\n",
    "\n",
    "    # iterative MCMC sampling\n",
    "    for _ in range(steps):\n",
    "        noise.normal_(0, 0.005)  # normally the stddev would be sqrt(2*eps), but this turned out to be a better value\n",
    "        # noise.normal_(0, np.sqrt(2*step_size))  # this noise is too big --> bad results\n",
    "        x_hat = torch.clamp(x_hat + noise, -1, 1)  # clip x_hat after applying noise\n",
    "\n",
    "        # calculate gradients w.r.t. input.\n",
    "        out = model(x_hat).sum()\n",
    "        grad, = torch.autograd.grad(out, x_hat, only_inputs=True)\n",
    "\n",
    "        # for stability (limits gradient value)\n",
    "        grad = torch.clamp(grad, -0.03, 0.03)\n",
    "\n",
    "        # Langevin step\n",
    "        x_hat = x_hat + step_size * grad\n",
    "\n",
    "        # clip x_hat after applying gradient\n",
    "        x_hat = torch.clamp(x_hat, -1, 1)\n",
    "\n",
    "        x_steps.append(x_hat.clone().detach())\n",
    "\n",
    "    # Reactivate gradients for the model parameters\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return torch.stack(x_steps, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel()\n",
    "\n",
    "loaded_state_dict = torch.load(\"saved_models/EBM.pt\", map_location=torch.device('cpu'))  # when you train with CUDA but evaluate on CPU\n",
    "model.load_state_dict(loaded_state_dict)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = torch.rand((10, 1, 28, 28)) * 2 - 1  # Start from uniform noise in (-1, 1) \n",
    "x_steps = generate_samples(model, x_0, steps=500, step_size=10)\n",
    "\n",
    "for k in range(x_steps.shape[1]):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=10)\n",
    "    fig.set_figheight(6)\n",
    "    fig.set_figwidth(14)\n",
    "    for i, x_step in enumerate(x_steps):\n",
    "        if i % 50 == 0:\n",
    "            idx = i // 50\n",
    "            ax[idx].imshow(x_step[k].view(28, 28).cpu().detach().numpy(), cmap='binary')\n",
    "            ax[idx].set_axis_off()\n",
    "    plt.draw()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
