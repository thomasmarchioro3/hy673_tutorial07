{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CNNModel, generate_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20         # number of training epochs\n",
    "batch_size = 128    # batch size\n",
    "lr=1e-4             # learning rate\n",
    "alpha=0.1           # regularization coeff\n",
    "steps = 60          # number of Langevin steps during training\n",
    "step_size = 10      # size of each Langevin steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "\n",
    "# normalize images between -1, 1\n",
    "# Min of input image = 0 -> 0-0.5 = -0.5 -> gets divided by 0.5 std -> -1\n",
    "# Max of input image = 255 -> toTensor -> 1 -> (1 - 0.5) / 0.5 -> 1\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)) \n",
    "                               ])\n",
    "\n",
    "train_dataset = MNIST(root='./data', train=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select elements with a certain probability\n",
    "\n",
    "Suppose we have an array of $d$ elements, and for each of them we want to select it with probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.rand(100)  # generate 10 random elements uniform in (0, 1)\n",
    "p = 0.05  # select with probability 5%\n",
    "\n",
    "selected_idx = np.argwhere(r < p)  # 5% probability of r[i] being lower than 0.05 for each i\n",
    "\n",
    "print(selected_idx)  # need to reshape it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training heuristic\n",
    "\n",
    "- Keep a dataset of generated examples during training\n",
    "- Take few Langevin steps at each training iteration (e.g., 60)\n",
    "- At every iteration, re-initialize each sample with 5% probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize examples at random\n",
    "# (1,) + x_shape = (1, 1, 28, 28)\n",
    "examples = [torch.rand((1,)+x_shape)*2 - 1 for _ in range(batch_size)]\n",
    "\n",
    "def generate_sample_train(model, steps=60, step_size=10):\n",
    "    # re-initialize each example with 5% probabiliy\n",
    "    r = np.random.rand(batch_size)\n",
    "    r_idx = np.argwhere(r < 0.05).reshape(-1,)\n",
    "    old_idx = np.argwhere(r > 0.05).reshape(-1,)\n",
    "    z = torch.rand((len(r_idx),) + x_shape) * 2 - 1  # create random examples\n",
    "    x_old = torch.cat([examples[idx] for idx in old_idx], dim=0)\n",
    "    x_new = torch.cat([z, x_old], dim=0).detach().to(device)\n",
    "\n",
    "    x_new = generate_samples(model, x_new, steps=steps, step_size=step_size)\n",
    "\n",
    "    examples[:] = list(x_new.to(torch.device(\"cpu\")).chunk(batch_size, dim=0))\n",
    "\n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel().to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: Contrastive divergence\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}[f_{\\theta}(\\text{Langevin}(Z))] - \\mathbb{E}[f_{\\theta}(X)]\n",
    "$$\n",
    "\n",
    "with\n",
    "$$\n",
    "X\\sim p_{\\text{data}}, \\text{ and } Z\\sim\\mathcal{U}(-1, 1)\n",
    "$$\n",
    "\n",
    "remembering that\n",
    "\n",
    "$$\n",
    "E_{\\theta}(x) \\approx e^{f_{\\theta} (x)} \\Rightarrow f_{\\theta} (x) \\approx \\log E_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "Intuition: Real samples should have high energy, while fake samples should have low energy \n",
    "\n",
    "### IMP: Loss behavior\n",
    "\n",
    "During training, the loss values will stabilize around a certain value.\n",
    "This is because: 1) the model learns to assign higher energy to the real samples and low energy to the fake samples, but at the same time 2) the generated fake samples will improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for x, _ in tqdm(train_loader):\n",
    "\n",
    "        opt.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "\n",
    "        x_hat = generate_sample_train(model, steps=steps, step_size=step_size)\n",
    "\n",
    "        # reshape images for Conv network\n",
    "        x = x.view((-1,)+x_shape)\n",
    "        x_hat = x_hat.view((-1,)+x_shape)\n",
    "\n",
    "        out_real = model(x)\n",
    "        out_fake = model(x_hat)\n",
    "\n",
    "        cd_loss = out_fake.mean() - out_real.mean()\n",
    "        reg_loss = (out_real ** 2).mean() + (out_fake ** 2).mean()\n",
    "        loss = cd_loss + alpha * reg_loss\n",
    "\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        opt.step()\n",
    "  \n",
    "    avg_loss = np.mean(losses)\n",
    "    print(f\"Epoch {epoch+1:03d}:{epochs:03d}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# if not os.path.isdir(\"saved_models\"):\n",
    "#     os.makedirs(\"saved_models\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"saved_models/EBM.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
